{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBanHYSSe61a",
        "outputId": "d16fd481-a1a6-4822-ac9c-5320b8122789"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Technical Design Documentation (Markdown as docstrings)\n",
        "Project Overview\n",
        "This project aims to create a system where each worker operates in its own isolated environment, processes its own data, and contributes to the overall machine learning task. The system will use multiprocessing to efficiently manage tasks and ensure data independence. The design will follow best practices for data analytics, machine learning, and isolated environments.</h1>**\n",
        "\n",
        "-------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "<h3>\n",
        "Components\n",
        "Isolated Environments\n",
        "Data Handling\n",
        "Task Management\n",
        "Multiprocessing\n",
        "Machine Learning\n",
        "Aggregation\n",
        "</h3>\n",
        "\n",
        "<h3>1. Isolated Environments\n",
        "Objective: Ensure each worker operates in its own isolated environment to prevent data leakage and dependency.\n",
        "\n",
        "Best Practices:\n",
        "> Use venv or virtualenv to create isolated environments.\n",
        "> Install only necessary dependencies in each environment.\n",
        "> Use environment variables to manage configurations.\n",
        "> Use bottom cell after #7 to run total file with your data loaded-in. Use '/content/sample_data' path for google colab\n",
        "> Implementation:</h3>"
      ],
      "metadata": {
        "id": "VWkg68EpMs1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install pandas numpy scikit-learn"
      ],
      "metadata": {
        "id": "VkU5Z7PeSezM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mFI9I1qMqjw"
      },
      "outputs": [],
      "source": [
        "## venv only, not google colab. Full file prog. at bottom.\n",
        "python -m venv worker_env_{worker_id}\n",
        "source worker_env_{worker_id}/bin/activate\n",
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>2. Data Handling\n",
        "Objective: Efficiently load, preprocess, and manage data for each worker.</h1>**\n",
        "\n",
        "**<h3>Best Practices:\n",
        "Store data in a structured format (e.g., JSON, CSV).\n",
        "Use efficient data structures (e.g., Pandas DataFrames) for data manipulation.\n",
        "Ensure data is cleaned and preprocessed before training.\n",
        "Implementation:</h3>**"
      ],
      "metadata": {
        "id": "JeEaHiZdNRqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Handling\n",
        "class DataHandler:\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "\n",
        "    def load_data(self):\n",
        "        if not os.path.exists(self.file_path):\n",
        "            print(f\"File {self.file_path} does not exist.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        data = []\n",
        "        with open(self.file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line and not line.startswith(\"Report Number\") and not line.startswith(\"Business Date\") and not line.startswith(\"Security ID\"):\n",
        "                    # Split the line into columns based on whitespace\n",
        "                    columns = line.split()\n",
        "                    data.append(columns)\n",
        "\n",
        "        if not data:\n",
        "            print(\"No valid data found in the file.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Determine the maximum number of columns in the data\n",
        "        max_columns = max(len(row) for row in data)\n",
        "\n",
        "        # Create a DataFrame from the parsed data\n",
        "        df = pd.DataFrame(data, columns=[f\"Column_{i}\" for i in range(max_columns)])\n",
        "        print(\"Loaded data columns:\", df.columns)\n",
        "        print(\"Loaded data shape:\", df.shape)\n",
        "        return df\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        # Implement preprocessing steps\n",
        "        # For example, ensure 'label' column exists\n",
        "        if 'label' not in df.columns:\n",
        "            df['label'] = np.random.randint(0, 1000, size=len(df))  # Dummy label for demo only\n",
        "\n",
        "        # Convert all columns to numeric, coercing errors to NaN\n",
        "        df = df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "        # Fill NaN values with 0 or any other appropriate value\n",
        "        df = df.fillna(0)\n",
        "\n",
        "        print(\"Preprocessed data columns:\", df.columns)\n",
        "        print(\"Preprocessed data shape:\", df.shape)\n",
        "        return df"
      ],
      "metadata": {
        "id": "9063c7kCOPwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>3.Task Management Objective: Efficiently manage and partition tasks across workers.</h1>**\n",
        "\n",
        "**<h3>Best Practices: Use a task queue to dynamically assign tasks. Ensure tasks are independent and can be processed in parallel.Implementation:</h3>**\n"
      ],
      "metadata": {
        "id": "M3iLXrFSOSaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task Management\n",
        "class TaskManager:\n",
        "    def __init__(self):\n",
        "        self.task_queue = Queue()\n",
        "\n",
        "    def add_task(self, task):\n",
        "        self.task_queue.put(task)\n",
        "\n",
        "    def get_task(self):\n",
        "        return self.task_queue.get()"
      ],
      "metadata": {
        "id": "nO1DSlC-Ox65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>4. Multiprocessing\n",
        "Objective: Use multiprocessing to efficiently manage and execute tasks.\n",
        "Best Practices:\n",
        "Use multiprocessing.Pool for managing worker processes.\n",
        "Ensure proper synchronization and communication between processes.\n",
        "Implementation:</h1>**"
      ],
      "metadata": {
        "id": "nIJEris2OzrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "# Worker Task\n",
        "def worker_task(worker_id):\n",
        "    env_path = setup_environment(worker_id)\n",
        "    data_handler = DataHandler(env_path)\n",
        "    data = data_handler.load_data()\n",
        "    if data.empty:\n",
        "        print(f\"No data loaded for worker {worker_id}\")\n",
        "        return None, None\n",
        "    preprocessed_data = data_handler.preprocess_data(data)\n",
        "    if preprocessed_data.empty:\n",
        "        print(f\"No data after preprocessing for worker {worker_id}\")\n",
        "        return None, None\n",
        "    model = MLModel(preprocessed_data)\n",
        "    monte_carlo = MonteCarloSimulation(model)\n",
        "    mean_accuracy, std_accuracy = monte_carlo.run_simulation(preprocessed_data)\n",
        "    return mean_accuracy, std_accuracy\n",
        "\n",
        "# Setup Environment\n",
        "def setup_environment(worker_id):\n",
        "    env_path = f'worker_env_{worker_id}'\n",
        "    os.makedirs(env_path, exist_ok=True)\n",
        "    return env_path\n",
        "\n",
        "## At bottom\n",
        "# # Main Function\n",
        "# def main():\n",
        "#     num_workers = 4\n",
        "#     with multiprocessing.Pool(num_workers) as pool:\n",
        "#         results = pool.map(worker_task, range(num_workers))\n",
        "#     print(f'Aggregated Results: {results}')\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     main()\n",
        "\n",
        "# parsed_trade = parse_trade(trade_example)\n",
        "# print(parsed_trade)"
      ],
      "metadata": {
        "id": "AXcktFq2O8Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>5. Machine Learning Objective: Train and test machine learning models efficiently.\n",
        "Best Practices:\n",
        "Use appropriate algorithms and hyperparameters.\n",
        "Ensure proper train-test split to avoid overfitting.\n",
        "Use cross-validation for model evaluation.\n",
        "Implementation:</h1>**"
      ],
      "metadata": {
        "id": "ds0voiREO_Pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Machine Learning Model\n",
        "class MLModel:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def train_model(self, data):\n",
        "        X = self.data.drop('label', axis=1)\n",
        "        y = self.data['label']\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "        model = RandomForestClassifier()\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        return accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "kfh7nH_zO_ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>6.Monte Carlo Simulation\n",
        "Objective: Perform Monte Carlo simulations to evaluate the robustness of trading strategies.\n",
        "Best Practices:\n",
        "Use vectorized operations for efficiency.\n",
        "Ensure reproducibility by setting random seeds.\n",
        "Use a large number of simulations to ensure statistical significance.\n",
        "Implementation:\n",
        "Implement a Monte Carlo simulation class that runs multiple simulations.\n",
        "Use vectorized operations with NumPy for efficiency.</h1>**"
      ],
      "metadata": {
        "id": "jYeoXPQZRELk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Monte Carlo Simulation\n",
        "class MonteCarloSimulation:\n",
        "    def __init__(self, model, num_simulations=1000):\n",
        "        self.model = model\n",
        "        self.num_simulations = num_simulations\n",
        "\n",
        "    def run_simulation(self, data):\n",
        "        results = []\n",
        "        for _ in range(self.num_simulations):\n",
        "            simulated_data = self.simulate_data(data)\n",
        "            accuracy = self.model.train_model(simulated_data)\n",
        "            results.append(accuracy)\n",
        "        return np.mean(results), np.std(results)\n",
        "\n",
        "    def simulate_data(self, data):\n",
        "        # Implement data simulation logic\n",
        "        return data"
      ],
      "metadata": {
        "id": "czpyu-wZREcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>7.Aggregation\n",
        "Objective: Aggregate results from each worker to form the final output.\n",
        "Best Practices:\n",
        "Use statistical methods to aggregate results.\n",
        "Ensure robustness and reliability of aggregated results.\n",
        "Implementation:</h1>**"
      ],
      "metadata": {
        "id": "F4HjzRz7PZpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Citation Machine: </h1>**\n",
        "\n",
        "**<h1>[1]Ccxt, CCXT/Python/CCXT at master Â· Author: Igor Kroitor 2024. Code Owners: *    @kroitor @frosty00 CCXT/CCXT, GitHub. (n.d.). https://github.com/ccxt/ccxt/tree/master/python/ccxt https://github.com/ccxt/ccxt/tree/master (accessed August 17, 2024).</h1>**\n",
        "\n",
        "**<h1>Data sample attribution to HitBtc API attribution. Citation for data and example test code to run: [1]About HITBTC API, API Documentation. (n.d.). https://api.hitbtc.com/ (accessed August 17, 2024). </h1>**"
      ],
      "metadata": {
        "id": "9If3W_HqX9pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def parse_trade(trade: dict, market: dict = None) -> dict:\n",
        "    \"\"\"\n",
        "    Example data from hitbtc3.py\n",
        "    \"\"\"\n",
        "    # Helper functions to safely extract values\n",
        "    def safe_string(d, key, default=None):\n",
        "        return str(d[key]) if key in d else default\n",
        "\n",
        "    def safe_value(d, key, default=None):\n",
        "        return d[key] if key in d else default\n",
        "\n",
        "    def parse8601(timestamp):\n",
        "        return datetime.strptime(timestamp, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "\n",
        "    # Extract values from the trade dictionary\n",
        "    timestamp = parse8601(safe_string(trade, 'timestamp'))\n",
        "    marketId = safe_string(trade, 'symbol')\n",
        "    market = market if market else {'symbol': marketId}\n",
        "    symbol = market['symbol']\n",
        "    fee = None\n",
        "    feeCostString = safe_string(trade, 'fee')\n",
        "    taker = safe_value(trade, 'taker')\n",
        "    takerOrMaker = 'taker' if taker else 'maker'\n",
        "\n",
        "    # Construct the parsed trade dictionary\n",
        "    parsed_trade = {\n",
        "        'timestamp': timestamp,\n",
        "        'datetime': timestamp.isoformat(),\n",
        "        'symbol': symbol,\n",
        "        'id': safe_string(trade, 'id'),\n",
        "        'order': safe_string(trade, 'order_id'),\n",
        "        'type': None,\n",
        "        'side': safe_string(trade, 'side'),\n",
        "        'price': float(safe_string(trade, 'price')),\n",
        "        'amount': float(safe_string(trade, 'quantity')),\n",
        "        'cost': float(safe_string(trade, 'price')) * float(safe_string(trade, 'quantity')),\n",
        "        'fee': {\n",
        "            'cost': float(feeCostString),\n",
        "            'currency': marketId.split('_')[1] if '_' in marketId else None,\n",
        "        } if feeCostString else None,\n",
        "        'takerOrMaker': takerOrMaker,\n",
        "        'info': trade,\n",
        "    }\n",
        "\n",
        "    return parsed_trade\n",
        "\n",
        "# Example data from hitbtc3.py\n",
        "trade_example = {\n",
        "    \"id\": 4718564,\n",
        "    \"order_id\": 58730811958,\n",
        "    \"client_order_id\": \"475c47d97f867f09726186eb22b4c3d4\",\n",
        "    \"symbol\": \"BTCUSDT_PERP\",\n",
        "    \"side\": \"sell\",\n",
        "    \"quantity\": \"0.0001\",\n",
        "    \"price\": \"41118.51\",\n",
        "    \"fee\": \"0.002055925500\",\n",
        "    \"timestamp\": \"2022-03-17T05:23:17.795Z\",\n",
        "    \"taker\": True,\n",
        "    \"position_id\": 2350122,\n",
        "    \"pnl\": \"0.002255000000\",\n",
        "    \"liquidation\": False\n",
        "}\n"
      ],
      "metadata": {
        "id": "N5io2il5ZfA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>def main():</h1>**"
      ],
      "metadata": {
        "id": "h7ZlDF6Ijst0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_results(results):\n",
        "    # Implement aggregation logic (e.g., averaging accuracies)\n",
        "    return sum(results) / len(results)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    num_workers = 4\n",
        "    with multiprocessing.Pool(num_workers) as pool:\n",
        "        results = pool.map(worker_task, range(num_workers))\n",
        "    print(f'Aggregated Results: {results}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "parsed_trade = parse_trade(trade_example)\n",
        "print(parsed_trade)"
      ],
      "metadata": {
        "id": "RDTiBiIyPgvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Full Simplest Toy Ex. Result</h1>**"
      ],
      "metadata": {
        "id": "m2FruoKyRV0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Load your data: json, txt, csv,... Make sure there's handling data for your specific case. Mainly handling for .json, then .txt</h1>**"
      ],
      "metadata": {
        "id": "FZV8_Alvk5gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "##### Verify code exists #####\n",
        "file_path = '/content/sample_data/report550_2023-12-04.txt'\n",
        "if os.path.exists(file_path):\n",
        "    print(f\"File {file_path} exists.\")\n",
        "else:\n",
        "    print(f\"File {file_path} does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VwOwXIkeKlA",
        "outputId": "bc0df524-7b06-44a7-ea76-881fb103b693"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File /content/sample_data/report550_2023-12-04.txt exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Need to upload your own data file (only within google colab)! Promise its worth it &#x1F600; (#hexadecimal) ! </h1>**"
      ],
      "metadata": {
        "id": "QW_1S29AbOO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "rThGMvCcj5-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "![17-facts-about-hexadecimal-reboot-1691980921](./17-facts-about-hexadecimal-reboot-1691980921.jpg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjUagJ1lciz4",
        "outputId": "d645825f-65b9-4215-b34f-459bb04cf412"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `./17-facts-about-hexadecimal-reboot-1691980921.jpg'\n",
            "/bin/bash: -c: line 1: `[17-facts-about-hexadecimal-reboot-1691980921](./17-facts-about-hexadecimal-reboot-1691980921.jpg)'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from queue import Queue\n",
        "\n",
        "# Data Handling\n",
        "class DataHandler:\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "\n",
        "    def load_data(self):\n",
        "        if not os.path.exists(self.file_path):\n",
        "            print(f\"File {self.file_path} does not exist.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        data = []\n",
        "        with open(self.file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line and not line.startswith(\"Report Number\") and not line.startswith(\"Business Date\") and not line.startswith(\"Security ID\"):\n",
        "                    # Split the line into columns based on whitespace\n",
        "                    columns = line.split()\n",
        "                    data.append(columns)\n",
        "\n",
        "        if not data:\n",
        "            print(\"No valid data found in the file.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Determine the maximum number of columns in the data\n",
        "        max_columns = max(len(row) for row in data)\n",
        "\n",
        "        # Create a DataFrame from the parsed data\n",
        "        df = pd.DataFrame(data, columns=[f\"Column_{i}\" for i in range(max_columns)])\n",
        "        print(\"Loaded data columns:\", df.columns)\n",
        "        print(\"Loaded data shape:\", df.shape)\n",
        "        return df\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        # Implement preprocessing steps\n",
        "        # For example, ensure 'label' column exists\n",
        "        if 'label' not in df.columns:\n",
        "            df['label'] = np.random.randint(0, 1000, size=len(df))  # Dummy label for demo only\n",
        "\n",
        "        # Convert all columns to numeric, coercing errors to NaN\n",
        "        df = df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "        # Fill NaN values with 0 or any other appropriate value\n",
        "        df = df.fillna(0)\n",
        "\n",
        "        print(\"Preprocessed data columns:\", df.columns)\n",
        "        print(\"Preprocessed data shape:\", df.shape)\n",
        "        return df\n",
        "\n",
        "# Machine Learning Model\n",
        "class MLModel:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def train_model(self, data):\n",
        "        X = self.data.drop('label', axis=1)\n",
        "        y = self.data['label']\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "        model = RandomForestClassifier()\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        return accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Monte Carlo Simulation\n",
        "class MonteCarloSimulation:\n",
        "    def __init__(self, model, num_simulations=1000):\n",
        "        self.model = model\n",
        "        self.num_simulations = num_simulations\n",
        "\n",
        "    def run_simulation(self, data):\n",
        "        results = []\n",
        "        for _ in range(self.num_simulations):\n",
        "            simulated_data = self.simulate_data(data)\n",
        "            accuracy = self.model.train_model(simulated_data)\n",
        "            results.append(accuracy)\n",
        "        mean_accuracy = np.mean(results)\n",
        "        std_accuracy = np.std(results)\n",
        "        normal_dist = np.random.normal(mean_accuracy, std_accuracy, self.num_simulations)\n",
        "        return mean_accuracy, std_accuracy, normal_dist\n",
        "\n",
        "    def simulate_data(self, data):\n",
        "        # Implement data simulation logic\n",
        "        return data\n",
        "\n",
        "# Task Management\n",
        "class TaskManager:\n",
        "    def __init__(self):\n",
        "        self.task_queue = Queue()\n",
        "\n",
        "    def add_task(self, task):\n",
        "        self.task_queue.put(task)\n",
        "\n",
        "    def get_task(self):\n",
        "        return self.task_queue.get()\n",
        "\n",
        "# Worker Task\n",
        "def worker_task(worker_id, file_path):\n",
        "    data_handler = DataHandler(file_path)\n",
        "    data = data_handler.load_data()\n",
        "    if data.empty:\n",
        "        print(f\"No data loaded for worker {worker_id}\")\n",
        "        return None, None, None\n",
        "    preprocessed_data = data_handler.preprocess_data(data)\n",
        "    if preprocessed_data.empty:\n",
        "        print(f\"No data after preprocessing for worker {worker_id}\")\n",
        "        return None, None, None\n",
        "    model = MLModel(preprocessed_data)\n",
        "    monte_carlo = MonteCarloSimulation(model)\n",
        "    mean_accuracy, std_accuracy, normal_dist = monte_carlo.run_simulation(preprocessed_data)\n",
        "    return mean_accuracy, std_accuracy, normal_dist\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "def parse_trade(trade: dict, market: dict = None) -> dict:\n",
        "    \"\"\"\n",
        "    Example data from hitbtc3.py\n",
        "    \"\"\"\n",
        "    # Helper functions to safely extract values\n",
        "    def safe_string(d, key, default=None):\n",
        "        return str(d[key]) if key in d else default\n",
        "\n",
        "    def safe_value(d, key, default=None):\n",
        "        return d[key] if key in d else default\n",
        "\n",
        "    def parse8601(timestamp):\n",
        "        return datetime.strptime(timestamp, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "\n",
        "    # Extract values from the trade dictionary\n",
        "    timestamp = parse8601(safe_string(trade, 'timestamp'))\n",
        "    marketId = safe_string(trade, 'symbol')\n",
        "    market = market if market else {'symbol': marketId}\n",
        "    symbol = market['symbol']\n",
        "    fee = None\n",
        "    feeCostString = safe_string(trade, 'fee')\n",
        "    taker = safe_value(trade, 'taker')\n",
        "    takerOrMaker = 'taker' if taker else 'maker'\n",
        "\n",
        "    # Construct the parsed trade dictionary\n",
        "    parsed_trade = {\n",
        "        'timestamp': timestamp,\n",
        "        'datetime': timestamp.isoformat(),\n",
        "        'symbol': symbol,\n",
        "        'id': safe_string(trade, 'id'),\n",
        "        'order': safe_string(trade, 'order_id'),\n",
        "        'type': None,\n",
        "        'side': safe_string(trade, 'side'),\n",
        "        'price': float(safe_string(trade, 'price')),\n",
        "        'amount': float(safe_string(trade, 'quantity')),\n",
        "        'cost': float(safe_string(trade, 'price')) * float(safe_string(trade, 'quantity')),\n",
        "        'fee': {\n",
        "            'cost': float(feeCostString),\n",
        "            'currency': marketId.split('_')[1] if '_' in marketId else None,\n",
        "        } if feeCostString else None,\n",
        "        'takerOrMaker': takerOrMaker,\n",
        "        'info': trade,\n",
        "    }\n",
        "\n",
        "    return parsed_trade\n",
        "\n",
        "# Example data from hitbtc3.py\n",
        "trade_example = {\n",
        "    \"id\": 4718564,\n",
        "    \"order_id\": 58730811958,\n",
        "    \"client_order_id\": \"475c47d97f867f09726186eb22b4c3d4\",\n",
        "    \"symbol\": \"BTCUSDT_PERP\",\n",
        "    \"side\": \"sell\",\n",
        "    \"quantity\": \"0.0001\",\n",
        "    \"price\": \"41118.51\",\n",
        "    \"fee\": \"0.002055925500\",\n",
        "    \"timestamp\": \"2022-03-17T05:23:17.795Z\",\n",
        "    \"taker\": True,\n",
        "    \"position_id\": 2350122,\n",
        "    \"pnl\": \"0.002255000000\",\n",
        "    \"liquidation\": False\n",
        "}\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    file_path = '/content/sample_data/report550_2023-12-04.txt'\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"File {file_path} does not exist.\")\n",
        "        return\n",
        "\n",
        "    num_workers = 4\n",
        "    with multiprocessing.Pool(num_workers) as pool:\n",
        "        results = pool.starmap(worker_task, [(i, file_path) for i in range(num_workers)])\n",
        "    print(f'Aggregated Results: {results}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "parsed_trade = parse_trade(trade_example)\n",
        "print(parsed_trade)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGHYRUoTiTSm",
        "outputId": "31389b80-905b-4ae0-9c3e-d30dcfcf1a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data columns: Index(['Column_0', 'Column_1', 'Column_2', 'Column_3', 'Column_4', 'Column_5',\n",
            "       'Column_6', 'Column_7', 'Column_8', 'Column_9', 'Column_10',\n",
            "       'Column_11', 'Column_12', 'Column_13', 'Column_14', 'Column_15',\n",
            "       'Column_16', 'Column_17'],\n",
            "      dtype='object')Loaded data columns:Loaded data columns:\n",
            " Loaded data columns:Loaded data shape: Index(['Column_0', 'Column_1', 'Column_2', 'Column_3', 'Column_4', 'Column_5',\n",
            "       'Column_6', 'Column_7', 'Column_8', 'Column_9', 'Column_10',\n",
            "       'Column_11', 'Column_12', 'Column_13', 'Column_14', 'Column_15',\n",
            "       'Column_16', 'Column_17'],\n",
            "      dtype='object') (19803, 18)\n",
            "Index(['Column_0', 'Column_1', 'Column_2', 'Column_3', 'Column_4', 'Column_5',\n",
            "       'Column_6', 'Column_7', 'Column_8', 'Column_9', 'Column_10',\n",
            "       'Column_11', 'Column_12', 'Column_13', 'Column_14', 'Column_15',\n",
            "       'Column_16', 'Column_17'],\n",
            "      dtype='object') Loaded data shape:\n",
            "\n",
            "Index(['Column_0', 'Column_1', 'Column_2', 'Column_3', 'Column_4', 'Column_5',\n",
            "       'Column_6', 'Column_7', 'Column_8', 'Column_9', 'Column_10',\n",
            "       'Column_11', 'Column_12', 'Column_13', 'Column_14', 'Column_15',\n",
            "       'Column_16', 'Column_17'],\n",
            "      dtype='object')Loaded data shape: \n",
            " (19803, 18)Loaded data shape:\n",
            "(19803, 18) \n",
            "(19803, 18)\n",
            "Preprocessed data columns: Index(['Column_0', 'Column_1', 'Column_2', 'Column_3', 'Column_4', 'Column_5',\n",
            "       'Column_6', 'Column_7', 'Column_8', 'Column_9', 'Column_10',\n",
            "       'Column_11', 'Column_12', 'Column_13', 'Column_14', 'Column_15',\n",
            "       'Column_16', 'Column_17', 'label'],\n",
            "      dtype='object')\n",
            "Preprocessed data shape: (19803, 19)\n",
            "Preprocessed data columns:Preprocessed data columns:Preprocessed data columns:  Index(['Column_0', 'Column_1', 'Column_2', 'Column_3', 'Column_4', 'Column_5',\n",
            "       'Column_6', 'Column_7', 'Column_8', 'Column_9', 'Column_10',\n",
            "       'Column_11', 'Column_12', 'Column_13', 'Column_14', 'Column_15',\n",
            "       'Column_16', 'Column_17', 'label'],\n",
            "      dtype='object') Index(['Column_0', 'Column_1', 'Column_2', 'Column_3', 'Column_4', 'Column_5',\n",
            "       'Column_6', 'Column_7', 'Column_8', 'Column_9', 'Column_10',\n",
            "       'Column_11', 'Column_12', 'Column_13', 'Column_14', 'Column_15',\n",
            "       'Column_16', 'Column_17', 'label'],\n",
            "      dtype='object')\n",
            "\n",
            "Preprocessed data shape:Index(['Column_0', 'Column_1', 'Column_2', 'Column_3', 'Column_4', 'Column_5',\n",
            "       'Column_6', 'Column_7', 'Column_8', 'Column_9', 'Column_10',\n",
            "       'Column_11', 'Column_12', 'Column_13', 'Column_14', 'Column_15',\n",
            "       'Column_16', 'Column_17', 'label'],\n",
            "      dtype='object') \n",
            "(19803, 19)Preprocessed data shape:\n",
            " Preprocessed data shape:(19803, 19) \n",
            "(19803, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Conclusion: This design ensures that each worker operates independently, processes its own data, and contributes to the overall machine learning task without data dependency issues. By following best practices for isolated environments, data handling, task management, multiprocessing, machine learning, and aggregation, we can achieve efficient and reliable data analytics.</h1>**"
      ],
      "metadata": {
        "id": "NjVlhWB_Pj9r"
      }
    }
  ]
}
